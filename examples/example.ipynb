{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`att_viz` 101**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`att_viz` is a Python package for visualizing self-attention. Here is a quick tutorial to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Running a basic experiment**\n",
    "\n",
    "In this experiment, we will load `Salesforce/codegen-350M-mono` and run inference on a single prompt.\n",
    "\n",
    "Because we opt for no attention aggregation, the HTML visualization will be \"chunked\", or broken down, into multiple HTML files.\n",
    "\n",
    "Let's take a look at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from att_viz.utils import Experiment\n",
    "from att_viz.renderer import Renderer, RenderConfig\n",
    "from att_viz.self_attention_model import SelfAttentionModel\n",
    "from att_viz.attention_aggregation_method import AttentionAggregationMethod\n",
    "\n",
    "\n",
    "### MODIFY HERE TO TEST OTHER MODELS OR PROMPTS ###\n",
    "\n",
    "model_name_or_directory: str = \"Salesforce/codegen-350M-mono\"\n",
    "prompt: str = \"print('Hello World')\"\n",
    "\n",
    "###################################################\n",
    "\n",
    "# Initialize the model: this loads the corresponding Huggingface model and tokenizer\n",
    "model = SelfAttentionModel(model_name_or_directory=model_name_or_directory)\n",
    "\n",
    "# Initialize the renderer with the base configuration and no attention aggregation method\n",
    "renderer = Renderer(\n",
    "    render_config=RenderConfig(), aggregation_method=AttentionAggregationMethod.NONE\n",
    ")\n",
    "\n",
    "experiment = Experiment(model, renderer)\n",
    "\n",
    "# This will run inference with the given prompt and save some html visualization files\n",
    "experiment.basic_experiment(prompt=prompt, aggr_method=AttentionAggregationMethod.NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. Running a headwise averaging experiment**\n",
    "\n",
    "##### **2.1. Why aggregate the self-attention values?**\n",
    "\n",
    "Well-designed aggregation methods could help pinpoint value patterns in the self-attention matrix.\n",
    "\n",
    "`att_viz` implements averaging across attention heads, and offers users the possibility to define their own aggregation functions by extending the `AttentionAggregationMethod` enumeration. \n",
    "\n",
    "##### **2.2. Using aggregation methods with `att_viz`**\n",
    "We can reuse the previous section's code, only switching the aggregation method to `HEADWISE_AVERAGING` in the experiment.\n",
    "\n",
    "Notice that this time the HTML visualization has not been \"chunked\" - it is already small enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY HERE TO TEST OTHER PROMPTS ###\n",
    "\n",
    "prompt: str = \"print('Hello World')\"\n",
    "\n",
    "#########################################\n",
    "\n",
    "experiment.basic_experiment(\n",
    "    prompt=prompt,\n",
    "    aggr_method=AttentionAggregationMethod.HEADWISE_AVERAGING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Running an experiment in two steps**\n",
    "\n",
    "Sometimes, it is preferable to run inference and processing separately. `att_viz` implements this by pairing `save_completions` with `process_saved_completions`.\n",
    "\n",
    "Here is how you can use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from att_viz.utils import save_completions, process_saved_completions\n",
    "from att_viz.renderer import RenderConfig\n",
    "from att_viz.attention_aggregation_method import AttentionAggregationMethod\n",
    "\n",
    "### MODIFY HERE TO TEST OTHER MODELS OR PROMPTS ###\n",
    "\n",
    "# N.B.: The model completion for prompt[i] will be saved in:\n",
    "# save_prefix[i]_{input_length, attention_matrix, completion}.pickle\n",
    "\n",
    "model_name_or_directory: str = \"Salesforce/codegen350M-mono\"\n",
    "prompts: list[str] = [\"print('Hello World')\"]\n",
    "save_prefixes: list[str] = [\"experiment_0\"]\n",
    "\n",
    "###################################################\n",
    "\n",
    "save_completions(\n",
    "    model_name_or_directory=model_name_or_directory,\n",
    "    prompts=prompts,\n",
    "    save_prefixes=save_prefixes,\n",
    ")\n",
    "\n",
    "process_saved_completions(\n",
    "    render_config=RenderConfig(),\n",
    "    aggregation_method=AttentionAggregationMethod.HEADWISE_AVERAGING,\n",
    "    save_prefixes=save_prefixes,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
